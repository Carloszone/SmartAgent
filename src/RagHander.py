from langchain_community.document_loaders import PyMuPDFLoader, TextLoader, UnstructuredWordDocumentLoader
from google import genai
from ConfigManager import config
import os
import re
from typing import List
from langchain_core.documents import Document
from ContentHandler import ContentHandler
import weaviate
import weaviate.classes as wvc
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
from google.genai.types import EmbedContentConfig
import weaviate
from weaviate.classes.config import Configure, Property, DataType
from weaviate.classes.query import Filter
from google.genai import types
import math


class RAGTool:
    """
    æ‰§è¡ŒRAGç›¸å…³æ“ä½œçš„ç±»
    """
    def __init__(self, agent_client=None, embeding_model=None, tokenizer=None, thread_num=8, batch_size=32):
        # åŠ è½½æ–‡æœ¬å†…å®¹å¤„ç†å™¨
        self.content_handler = ContentHandler()

        # æ¨¡å‹å®¢æˆ·ç«¯
        # self.model_client = self.content_handler.client
        self.model_client = genai.Client(api_key=config.get_setting('model_api_key'))

        # æ¨¡å‹ä¿¡æ¯
        self.generative_model_name = config.get_setting('generative_model_name')
        self.embeding_model_name = config.get_setting('embeding_model_name')

        # å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.vector_database_client = None

        # å¤„ç†å‚æ•°
        self.thread_num = thread_num   # çº¿ç¨‹æ•°
        self.batch_size = batch_size   # æ‰¹æ¬¡è§„æ¨¡
        
        # æ–‡æ¡£ç±»å‹-è§£æå·¥å…·æ˜ å°„è¡¨
        self.documente_loader_mapping = {
            ".pdf": PyMuPDFLoader,
            ".docx": UnstructuredWordDocumentLoader
        }

        # é»˜è®¤æ–‡æœ¬æ–‡ä»¶çš„è§£æå·¥å…·
        self.default_text_reader = TextLoader
        

        # è‡ªæè¿°
        self.tool_description = self.self_description()

    def connect_to_weative_database(self):
        """
        åˆ›å»ºä¸€ä¸ªåˆ°weativeæ•°æ®åº“çš„è¿æ¥
        """
        # å‚æ•°æå–
        vector_database_config = config.get_setting('vector_database_config')
        vector_database_host = vector_database_config.get('host')
        vector_database_port = vector_database_config.get('port')
        vector_database_grpc_port = vector_database_config.get('grpc_port')

        # æ„å»ºè¿æ¥
        try:
            self.vector_database_client = weaviate.connect_to_custom(
                http_host=vector_database_host,
                http_port=int(vector_database_port),
                http_secure=False,
                grpc_host='localhost',
                grpc_port=int(vector_database_grpc_port),
                grpc_secure=False,
            )

            # æ£€æŸ¥è¿æ¥æ˜¯å¦æˆåŠŸ
            self.vector_database_client.is_ready() # æ£€æŸ¥è¿æ¥æ˜¯å¦æˆåŠŸ
            print(f'è¿æ¥åˆ°weativeæ•°æ®åº“æˆåŠŸ')
            return self.vector_database_client
        except Exception as e:
            print(f"Failed to connect to Weaviate: {e}")
            raise

    def save_to_vector_database(self, data_list: List[dict] = None):
        """
        å°†æ•°æ®å­˜å…¥æ•°æ®åº“
        """
        if data_list is None:
            print(f'æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜')
            return
        else:
            # æå–é…ç½®ä¿¡æ¯
            vector_database_collection_name = config.get_setting('vector_database_config').get('knewledge_base_collection_name')

            # æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
            if not self.vector_database_client.collections.exists(vector_database_collection_name):
                raise Exception(f'å‘é‡æ•°æ®åº“ä¸­ä¸å­˜åœ¨{vector_database_collection_name}æ•°æ®è¡¨')
            else:
                # éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
                if not self.vector_database_client.collections.exists(vector_database_collection_name):
                    print(f'æ•°æ®è¡¨{vector_database_collection_name}ä¸å­˜åœ¨')
                    raise
                else:
                    document_collection = self.vector_database_client.collections.get(vector_database_collection_name)
                    aggregation_result = document_collection.aggregate.over_all(total_count=True)
                    total = aggregation_result.total_count
                    print(f"\nğŸ“Š æ•°æ®è¡¨ '{vector_database_collection_name}' çš„æ€»è®°å½•æ•°ä¸º: {total}")
                # å°è¯•ä¿å­˜æ•°æ®
                try:
                    print(f"\nå‡†å¤‡æ’å…¥ {len(data_list)} æ¡æ•°æ®...")

                    with document_collection.batch.dynamic() as batch:
                        for item in data_list:
                            my_vector=item.get("vector")

                            # å†…éƒ¨æ£€æŸ¥
                            if my_vector is None or any(v is None for v in my_vector):
                                print("é”™è¯¯ï¼šå‘é‡ä¸º None æˆ–å†…éƒ¨åŒ…å« None å€¼ï¼Œè·³è¿‡æ­¤æ¡æ•°æ®ã€‚")
                                continue

                            # æ£€æŸ¥ NaN
                            if any(math.isnan(v) for v in my_vector):
                                print("é”™è¯¯ï¼šå‘é‡ä¸­åŒ…å« NaN å€¼ï¼Œè·³è¿‡æ­¤æ¡æ•°æ®ã€‚")

                            # æ£€æŸ¥æ— ç©·å¤§
                            if any(math.isinf(v) for v in my_vector):
                                print("é”™è¯¯ï¼šå‘é‡ä¸­åŒ…å«æ— ç©·å¤§å€¼ï¼Œè·³è¿‡æ­¤æ¡æ•°æ®ã€‚")

                            try:
                                clean_vector = [float(v) for v in my_vector]
                            except (ValueError, TypeError):
                                print("é”™è¯¯ï¼šå‘é‡ä¸­çš„æŸä¸ªå€¼æ— æ³•è¢«è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚")

                            batch.add_object(properties=item.get("properties"),
                                             vector=item.get("vector"))

                    # æ£€æŸ¥æ‰¹é‡æ“ä½œä¸­æ˜¯å¦æœ‰é”™è¯¯
                    if document_collection.batch.failed_objects:
                        print(f"æ’å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {document_collection.batch.failed_objects}")
                except Exception as e:
                    print(f"æ’å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    raise

    def close_weavier_connection(self):
        """
        ä¸€ä¸ªä¸“ç”¨çš„å…³é—­æ–¹æ³•ï¼Œç”¨äºåœ¨åº”ç”¨é€€å‡ºå‰é‡Šæ”¾èµ„æºã€‚
        """
        print("æ–­å¼€å’ŒWeavieræ•°æ®åº“çš„è¿æ¥")
        if self.vector_database_client:
            self.vector_database_client.close()

    def get_weavier_collection_info(self):
        pass

    def self_description(self):
        """
        ç”¨äºç”Ÿæˆè‡ªèº«è¯´æ˜ä¹¦çš„å‡½æ•°ï¼Œè§£é‡Šè‡ªèº«åŠŸèƒ½ï¼Œä¾›agentä½¿ç”¨
        """
    
    def document_reader(self, file_path) -> List:
        """
        ç”¨äºè¯»å–å¯¼å…¥çš„æ–‡æ¡£çš„å‡½æ•°
        """
        # è·å–æ–‡ä»¶åç¼€
        file_extension = os.path.splitext(file_path)[1].lower()

        # è¿›è¡Œåç¼€åŒ¹é…
        if file_extension in self.documente_loader_mapping:
            document_loader = self.documente_loader_mapping.get(file_extension, self.default_text_reader)

            # åŸºäºåç¼€ï¼Œåˆ›å»ºloaderå®ä¾‹
            if file_extension in self.documente_loader_mapping.keys():
                loader = document_loader(file_path)
            else:
                loader = document_loader(file_path, encoding='utf-8')

            # åŠ è½½å¹¶è¿”å›æ–‡æ¡£å†…å®¹
            return loader.load()
        else:
            print(f"è­¦å‘Šï¼šä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ {file_extension}ï¼Œè·³è¿‡æ–‡ä»¶ {file_path}")
            return []

    def clean_text(self, docs: List[Document]) -> List[Document]:
        """
        æ–‡æœ¬æ¸…æ´—å‡½æ•°
        """
        new_docs = []  # å‚¨å­˜æ¸…æ´—åçš„æ–‡æ¡£

        for doc in docs:
            # æ–‡æ¡£å¤åˆ¶
            content = doc.page_content 

            # (è‹±æ–‡å†…å®¹)æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºä¸€ä¸ª
            content = re.sub(r'\s+', ' ', content)

            # (è‹±æ–‡å†…å®¹)æ›¿æ¢å¤šä¸ªæ¢è¡Œç¬¦ä¸ºä¸€ä¸ª
            content = re.sub(r'\n+', '\n', content)

            # (è‹±æ–‡å†…å®¹)å»é™¤æ–‡æ¡£ä¸­çš„è¿å­—ç¬¦
            content = re.sub(r'-\s*\n', '', content)

            # (ä¸­æ–‡å†…å®¹)ç§»é™¤ä¸­æ–‡æ–‡æœ¬ä¹‹é—´çš„å¤šä½™ç©ºæ ¼
            ontent = re.sub(r'([\u4e00-\u9fa5])\s+([\u4e00-\u9fa5])', r'\1\2', content)

            # åˆå¹¶è¢«é”™è¯¯åˆ‡åˆ†çš„æ®µè½æˆ–å¥å­
            content = re.sub(r'(?<![ã€‚ï¼ï¼Ÿ\n])\n', '', content)
            content = re.sub(r'\n+', '\n', content)

            # å»é™¤é¡µçœ‰é¡µè„šå’Œé¡µç ç­‰å™ªéŸ³ä¿¡æ¯
            content = re.sub(r'(?i)ç¬¬\s*\d+\s*é¡µ', '', content)
            content = re.sub(r'(?i)page\s*\d+', '', content)

            # ç§»é™¤é‡å¤æ€§çš„ä¹±ç å™ªå£°
            noise_pattern = r"([\s`\\/â€™'\'Vä¸¶ã€()]+){5,}"
            content = re.sub(noise_pattern, ' ', content)
            content = re.sub(r'\s+', ' ', content).strip()

            # å»é™¤æ— æ³•è¯†åˆ«çš„ä¹±ç ä¸å­—ç¬¦
            allowed_chars = re.compile(
                r'[^\u4e00-\u9fa5'  # ä¸­æ—¥éŸ©ç»Ÿä¸€è¡¨æ„æ–‡å­—
                r'a-zA-Z0-9'       # å­—æ¯å’Œæ•°å­—
                r'\s'              # ç©ºç™½ç¬¦ (åŒ…æ‹¬ç©ºæ ¼, \n, \t)
                r'ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ã€Šã€‹ã€ã€‘ï½›ï½â€œâ€â€™ã€' # ä¸­æ–‡æ ‡ç‚¹
                r',.?!:;()\[\]{}<>"\'~`@#$%^&*-_=+|\\/' # è‹±æ–‡æ ‡ç‚¹å’Œç¬¦å·
                r']'
            )
            content = allowed_chars.sub('', content)

            # å»é™¤å¼€å¤´å’Œç»“å°¾å¤„çš„ç©ºç™½å­—ç¬¦
            content = content.strip()

            # æ„å»ºä¸€ä¸ªæ–°çš„Documentå¯¹è±¡
            new_doc = Document(page_content=content, metadata=doc.metadata)
            new_docs.append(new_doc)
        return new_docs

    def text_window_retrieval(self, docs):
        """
        åˆ©ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯è¿›è¡Œå†…å®¹æ‹¼æ¥
        """
        page_lens = len(docs)
        window_docs = []
        for index in range(page_lens):
            if index == 0:
                if page_lens >= 2:
                    window_docs.append(
                        {
                            'target_content': docs[index],
                            'below_content': docs[index + 1]
                        }
                    )
                else:
                    window_docs.append(
                        {
                            'target_content': docs[index]
                        }
                    )
            elif index == page_lens - 1:
                window_docs.append(
                    {
                        'above_content': docs[index - 1],
                        'target_content': docs[index]
                    }
                )
            else:
                window_docs.append(
                    {
                        'above_content': docs[index - 1],
                        'target_content': docs[index],
                        'below_content': docs[index + 1]
                    }
                )
        return window_docs

    def markdown_formatter_single_flow(self, contents):
        """
        å¯¹è¾“å…¥çš„æ–‡æ¡£å†…å®¹è¿›è¡Œmarkdownæ ¼å¼åŒ–
        """
        # è¾“å…¥è§£æ
        output_contents = []
        for content in contents:
            # è¾“å…¥è§£æ
            text = {key: value.page_content for key, value in content.items() if isinstance(value, Document)}

            # promptç”Ÿæˆï¼ˆæ–‡æœ¬markdownæ ¼å¼åŒ–ï¼‰
            output_prompt = self.content_handler.prompt_generator(content=text, mode="content_markdown")

            # æ‰§è¡Œprompt
            response = self.content_handler.client.models.generate_content(model=self.generative_model_name, contents=output_prompt)

            # æå–è¾“å‡ºjson
            output_json = self.content_handler.json_extractor(response.text)

            # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æ¡£å†…å®¹
            output_content = {key: Document(page_content=value, metadata=content[key].metadata) for key, value in output_json.items()if len(value) > 0}

            output_contents.append(output_content)
        return output_contents
    async def markdown_formatter(self, content: dict):
        """
        å¯¹è¾“å…¥çš„æ–‡æ¡£å†…å®¹è¿›è¡Œmarkdownæ ¼å¼åŒ–
        """
        # è¾“å…¥è§£æ
        text = {key: value.page_content for key, value in content.items() if isinstance(value, Document)}

        # promptç”Ÿæˆï¼ˆæ–‡æœ¬markdownæ ¼å¼åŒ–ï¼‰
        output_prompt = self.content_handler.prompt_generator(content=text, mode="content_markdown")

        # æ‰§è¡Œprompt
        response = await self.content_handler.client.aio.models.generate_content(model=self.generative_model_name, 
                                                                                 contents=output_prompt,
                                                                                 config=types.GenerateContentConfig(temperature=0.0))

        # æå–è¾“å‡ºjson
        output_json = self.content_handler.json_extractor(response.text)

        # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æ¡£å†…å®¹
        output_content = {key: Document(page_content=value, metadata=content[key].metadata) for key, value in output_json.items()if len(value) > 0}
        return output_content

    async def markdown_formatter_async(self, contents: List[dict]) -> List[dict]:
        """
        å¯¹è¾“å…¥çš„æ–‡æ¡£å†…å®¹è¿›è¡Œmarkdownæ ¼å¼åŒ–(å¼‚æ­¥å¤„ç†)
        """
        # print(f'å¼€å§‹è¿›è¡Œæ–‡æ¡£æ ¼å¼åŒ–çš„è¾“å…¥ï¼š', contents)
        tasks = [self.markdown_formatter(content) for content in contents]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        # print(f'æ–‡æ¡£æ ¼å¼åŒ–çš„è¾“å‡ºï¼š', results)

        # è¿‡æ»¤æ‰å¤„ç†å¤±è´¥çš„ç»“æœ
        # successful_results = [res for res in results if isinstance(res, Document)]
        return results

    async def text_chunk_splitter(self, content: dict):
        """
        åŸºäºmarkdownæ ¼å¼çš„åˆ†æœ¬åˆ†å—å‡½æ•°
        """
        # è¾“å…¥è§£æ
        text = {key: value.page_content for key, value in content.items()}

        # promptç”Ÿæˆï¼ˆæ–‡æœ¬markdownæ ¼å¼åŒ–ï¼‰
        output_prompt = self.content_handler.prompt_generator(content=text, mode="chunker")

        # æ‰§è¡Œprompt
        response = await self.content_handler.client.aio.models.generate_content(model=self.generative_model_name, 
                                                                                 contents=output_prompt,
                                                                                 config=types.GenerateContentConfig(temperature=0.0))
 
        # æå–è¾“å‡ºjson
        output_json = self.content_handler.json_extractor(response.text)

        # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æ¡£å†…å®¹
        formatted_chunkers = []
        for index, output_content in enumerate(output_json['chunks']):
            chunk_metadata = content['target_content'].metadata
            chunk_metadata['chunk_seq_id'] = index
            chunk_document = Document(page_content=output_content, metadata=chunk_metadata)
            formatted_chunkers.append(chunk_document)
        return formatted_chunkers

    async def text_chunk_splitter_async(self, contents: List[dict]) -> List[List[Document]]:
        """
        åŸºäºmarkdownæ ¼å¼çš„æ–‡æœ¬åˆ†å—å‡½æ•°(æ‰¹é‡)
        """
        # print(f'æ–‡æ¡£åˆ†å—è¾“å…¥å†…å®¹ï¼š', contents)
        tasks = [self.text_chunk_splitter(content) for content in contents]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        # print(f'æ–‡æ¡£åˆ†å—è¾“å‡ºå†…å®¹ï¼š', results)
        
        # è¿‡æ»¤æ‰å¤„ç†å¤±è´¥çš„ç»“æœ
        # successful_results = [res for res in results if isinstance(res, Document)]
        return results

    def token_calculator(self, content):
        """
        è®¡ç®—è¾“å…¥çš„tokenä½¿ç”¨é‡çš„å‡½æ•°,tokenizerå¿…é¡»å’Œembedingæ¨¡å‹ä»¥åŠç”Ÿæˆæ¨¡å‹é…å¥—
        """
        response = self.model_client.models.count_tokens(model=self.generative_model_name,
                                                         contents=content)
        return response.total_tokens
    
    async def text_summary_asyn(self, content):
        """
        å¯¹æ–‡æ¡£å—å†…å®¹è¿›è¡Œç²¾ç®€å’Œæ¦‚æ‹¬çš„å‡½æ•°
        """
        # è¾“å…¥è§£æ
        text = {"summary": content.page_content}

        if self.token_calculator(text["summary"]) > 200:
            # promptç”Ÿæˆï¼ˆæ–‡æœ¬markdownæ ¼å¼åŒ–ï¼‰
            output_prompt = self.content_handler.prompt_generator(content=text, mode="text_summary")

            # æ‰§è¡Œprompt
            response = await self.content_handler.client.aio.models.generate_content(model=self.generative_model_name, 
                                                                                     contents=output_prompt,
                                                                                     config=types.GenerateContentConfig(temperature=0.0))

            # æå–è¾“å‡ºjson
            output_json = self.content_handler.json_extractor(response.text)
        else:
            output_json = text

        # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æ¡£å†…å®¹
        output_content = Document(page_content=output_json['summary'], metadata=content.metadata)

        return output_content

    async def text_summary_async(self, contents):
        """
        å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œç²¾ç®€å’Œæ¦‚æ‹¬çš„å‡½æ•°(æ‰¹é‡)
        """
        # for content in contents:
        #     for chunks in content:
        #         print(f'æ–‡æ¡£æ¦‚æ‹¬è¾“å…¥å†…å®¹ï¼š', type(chunks), chunks.page_content)
        tasks = [self.text_summary_asyn(chunk_content) for content in contents for chunk_content in content]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        # for res in results:
        #     print(f'æ–‡æ¡£æ¦‚æ‹¬è¾“å‡ºç±»å‹ï¼š', type(res))
        #     print(f'æ–‡æ¡£æ¦‚æ‹¬è¾“å‡ºå†…å®¹ï¼š', res.page_content)

        # è¿‡æ»¤æ‰å¤„ç†å¤±è´¥çš„ç»“æœ
        # print('æ¦‚æ‹¬è¾“å‡ºç»“æœæ•°é‡', len(results))
        successful_results = [res for res in results if isinstance(res, Document)]
        return successful_results 

    async def text_embeding_async(self, contents):
        """
        æ¥å—ä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨ï¼Œå¹¶æ‰¹é‡ç”Ÿæˆembeidngs
        """
        if not contents:
            return []

        try:
            # è¾“å…¥è§£æ
            texts_to_embed = [content.page_content for content in contents]
            print('æ–‡æ¡£åˆ†å—æ•°ç›®ï¼š', len(texts_to_embed))

            # è·å–embeding
            response = await self.model_client.aio.models.embed_content(model=self.embeding_model_name,
                                                                    contents=texts_to_embed,
                                                                    config=EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                                                                )
            print('embeddingæ•°é‡', len(response.embeddings), type(response.embeddings))
            return response
        except Exception as e:
            print(f"Embeddingè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªä¸è¾“å…¥é•¿åº¦ç›¸åŒ¹é…çš„ç©ºå‘é‡åˆ—è¡¨æˆ–è¿›è¡Œå…¶ä»–é”™è¯¯å¤„ç†
            return [[] for _ in contents]

    def text_embeding(self, contents):
        """
        æ¥å—ä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨ï¼Œå¹¶æ‰¹é‡ç”Ÿæˆembeidngs
        """
        print(f'embedingè¾“å…¥çš„å†…å®¹ä¸ºï¼š', contents)
        if not contents:
            return []

        try:
            # è¾“å…¥è§£æ
            texts_to_embed = [content.page_content for content in contents] * 3
            print('æ–‡æ¡£åˆ†å—æ•°ç›®ï¼š', len(texts_to_embed))

            # è·å–embeding
            response = self.model_client.models.embed_content(model=self.embeding_model_name,
                                                                    contents=texts_to_embed,
                                                                    config=EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                                                                )
            print('embeddingæ•°é‡', len(response.embeddings), type(response.embeddings))
            return response.embeddings
        except Exception as e:
            print(f"Embeddingè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªä¸è¾“å…¥é•¿åº¦ç›¸åŒ¹é…çš„ç©ºå‘é‡åˆ—è¡¨æˆ–è¿›è¡Œå…¶ä»–é”™è¯¯å¤„ç†
            return [[] for _ in contents]

    def save_data_generator(self, contents, embeddings):
        """
        ç”Ÿæˆç”¨äºä¿å­˜åˆ°å‘é‡æ•°æ®åº“çš„dictå¯¹è±¡
        """
        data_list = []
        for content, embedding in zip(contents, embeddings.embeddings):
            # print(f'ä¼ å…¥çš„å†…å®¹å˜é‡ç±»å‹ï¼š', type(content))
            # print(f'ä¼ å…¥çš„å†…å®¹å˜é‡ä¿¡æ¯ï¼š', type(embedding))
            # print(f'ä¼ å…¥çš„å‘é‡å˜é‡ç±»å‹ï¼š', type(content))
            # print(f'ä¼ å…¥çš„å‘é‡å˜é‡ä¿¡æ¯ï¼š', type(embedding))
            text = content.page_content
            metadata = content.metadata

            # æ„å»ºå‚¨å­˜å­—å…¸
            dict_for_storage = {
                "vector": embedding.values,
                'text': text,
                "properties":{
                    "metadata": {
                        # æ¥æºä¸æº¯æºä¿¡æ¯
                        # "file_id": '001', æ–‡ä»¶idåœ¨å‚¨å­˜æ—¶è‡ªåŠ¨ç”Ÿæˆ
                        "source": metadata.get("source", ''),

                        # è¿‡æ»¤å’Œæ£€ç´¢ä¿¡æ¯
                        "document_type": metadata.get("document_type", "pdf"),
                        "access_level": metadata.get("access_level", 0),

                        # å†…å®¹ä¸ç»“æ„ä¿¡æ¯
                        "page_number": metadata.get("page_number", -1),
                        "chunk_seq_id": metadata.get("chunk_seq_id", -1)

                        # ç»´æŠ¤ä¸ç‰ˆæœ¬ä¿¡æ¯
                        # "file_hash": "",
                        # "file_short_hash": "",
                        # "version": "v2.2"
                    }

                }
            }

            data_list.append(dict_for_storage)
        return data_list
    
    def output_rerank(self):
        """
        å¯¹è¿”å›çš„å†…å®¹è¿›è¡Œé‡è¦æ€§é‡æ’åº
        """
        pass
    
    async def document_to_vector_database_handler_async(self, file_path: str, metadata_dict: dict):
        """
        è´Ÿè´£å•ä¸ªæ–‡ä»¶çš„ETFæµç¨‹ã€‚è¿™ä¸ªå‡½æ•°å†…éƒ¨åŒ…å«äº†æ‰€æœ‰çš„å¼‚æ­¥å’Œæ‰¹é‡æ“ä½œ
        """
        try:
            # 1. å‡†å¤‡å·¥ä½œï¼šè¯»å–æ–‡ä»¶ï¼Œæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—
            print('***å¼€å§‹è¯»å–æ–‡ä»¶')
            raw_docs = self.document_reader(file_path)
            print(f'###è¯»å–æ–‡ä»¶å®Œæˆï¼Œå…±{len(raw_docs)}ä¸ªæ–‡æ¡£')

            print('***æ·»åŠ å…ƒæ•°æ®åˆ°doc')
            if metadata_dict:
                for key, value in metadata_dict.items():
                    raw_docs.metadata[key] = value

            print('***å¼€å§‹æ–‡æœ¬æ¸…æ´—')
            cleaned_docs = self.clean_text(raw_docs)
            print(f'###æ–‡æœ¬æ¸…æ´—å®Œæˆï¼Œå…±{len(cleaned_docs)}ä¸ªæ–‡æ¡£')

            print('***å¼€å§‹æ–‡æœ¬æ»‘åŠ¨çª—å£æ‹¼æ¥')
            window_contexts = self.text_window_retrieval(cleaned_docs)
            print(f'###æ–‡æœ¬æ»‘åŠ¨çª—å£æ‹¼æ¥å®Œæˆï¼Œå…±{len(window_contexts)}ä¸ªæ–‡æ¡£')

            # 2. å¹¶å‘Markdownæ ¼å¼åŒ–
            print('***å¼€å§‹markdownæ ¼å¼åŒ–')
            markdown_contents = await self.markdown_formatter_async(window_contexts)
            # markdown_contents = self.markdown_formatter_single_flow(window_contexts)
            print(f'#### markdownæ ¼å¼åŒ–å®Œæˆï¼Œå…±{len(markdown_contents)}ä¸ªæ–‡æ¡£')

            # 3. å¹¶å‘æ–‡æœ¬åˆ†å—
            print('***å¼€å§‹æ–‡æœ¬åˆ†å—')
            chunked_contents = await self.text_chunk_splitter_async(markdown_contents)
            print(f'### æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…±{len(chunked_contents)}ä¸ªæ–‡æ¡£')

            # 4. å¹¶å‘æ–‡æœ¬æ¦‚æ‹¬
            print('***å¼€å§‹æ–‡æœ¬æ¦‚æ‹¬')
            summary_contents = await self.text_summary_async(chunked_contents)
            print(f'### æ–‡æœ¬æ¦‚æ‹¬å®Œæˆï¼Œå…±{len(summary_contents)}ä¸ªæ–‡æ¡£')

            # 5. å¹¶å‘æ–‡æœ¬embeding
            print('***å¼€å§‹æ–‡æœ¬embeding')
            embeded_contents = await self.text_embeding_async(summary_contents)
            # embeded_contents = self.text_embeding(summary_contents)
            print(f'###æ–‡æœ¬embedingå®Œæˆï¼Œå…±{len(embeded_contents.embeddings)}ä¸ªåµŒå…¥å‘é‡')

            # 6. ç”Ÿæˆä¿å­˜ç”¨çš„æ•°æ®
            print(f'***å¼€å§‹ä¿å­˜æ•°æ®')
            records_to_save = self.save_data_generator(summary_contents, embeded_contents)
            print(f'###ç”Ÿæˆä¿å­˜ç”¨çš„æ•°æ®å®Œæˆï¼Œå…±{len(records_to_save)}ä¸ªæ–‡æ¡£')

            # 7. å¹¶å‘ä¿å­˜æ•°æ®
            print(f'***å¼€å§‹ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“')
            await asyncio.to_thread(self.save_to_vector_database, records_to_save)

            print(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_path}")
            return {"status": "success", "file_path": file_path}
        except Exception as e:
            print(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return {"status": "failed", "file_path": file_path, "error": str(e)}        

    def document_to_vector_database_handler(self, file_paths: List[str], metadata: List[dict] = None):
        """
        æ–‡æ¡£å¤„ç†å‡½æ•°,å°†ä¼ å…¥çš„æ–‡æ¡£æ ¼å¼åŒ–ï¼Œåˆ†å—ï¼Œæç‚¼å¹¶embedingåŒ–å­˜å…¥å‘é‡æ•°æ®åº“
        """
        # è·å–æ•°æ®åº“è¿æ¥
        self.connect_to_weative_database()

        # å¤šçº¿ç¨‹å¤„ç†
        if metadata is None:
            metadata = [None] * len(file_paths)

        with ThreadPoolExecutor(max_workers=self.thread_num) as executor:
            futures = [executor.submit(asyncio.run, self.document_to_vector_database_handler_async(path, metadata_dict)) for path, metadata_dict in zip(file_paths, metadata)]

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
            for future in futures:
                result = future.result()
                print(f"ä»»åŠ¡å®Œæˆ: {result}")

        # å…³é—­æ•°æ®åº“è¿æ¥
        self.close_weavier_connection()

    def chat_to_vector_database_handler(self, file_path: str):
        pass

    def search_knowledge_base(self, request):
        """è¯·æ±‚å‘é‡æ•°æ®åº“,ä»çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        pass

    def serach_chat_history(self, request):
        """è¯·æ±‚å‘é‡æ•°æ®åº“,ä»èŠå¤©è®°å½•ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
        pass

if __name__ == '__main__':
    file_path = 'data/Documents/å®¤å†…ç©ºæ°”è´¨é‡æ£€æµ‹æŠ¥å‘Š.pdf'
    test_tool = RAGTool()

    test_tool.document_to_vector_database_handler(file_paths=[file_path])

